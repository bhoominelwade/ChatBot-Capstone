!pip install python-dotenv
!pip install -U torch transformers
!pip install pypdf
!pip install tiktoken
!pip install -U langchain-community
!pip install -U sentence-transformers
!pip install faiss-cpu



import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
#ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")  # Reads .env file with ACCESS_TOKEN=<your hugging face access token>
ACCESS_TOKEN ='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'
# Model ID and Tokenizer
model_id = "google/gemma-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_id, token=ACCESS_TOKEN)

# Load the model for CPU (no quantization for CPU)
model = AutoModelForCausalLM.from_pretrained(model_id, token=ACCESS_TOKEN)
model.eval()

# Set device to CPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)  # Ensure the model is on the correct device

print(f"Using {device}")

# Example code to test model (optional)
# input_text = "Hello, how are you?"
# inputs = tokenizer(input_text, return_tensors="pt")
# outputs = model.generate(**inputs)
# print(tokenizer.decode(outputs[0], skip_special_tokens=True))



def inference(question: str, context: str):

    if context == None or context == "":
        prompt = f"""Give a detailed answer to the following question. Question: {question}"""
    else:
        prompt = f"""Using the information contained in the context, give a detailed answer to the question.
            Context: {context}.
            Question: {question}"""
    chat = [
        {"role": "user", "content": prompt},
        # { "role": "model", "content": "Recurrent Attention (RAG)** is a novel neural network architecture specifically designed" }
    ]
    formatted_prompt = tokenizer.apply_chat_template(
        chat,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = tokenizer.encode(
        formatted_prompt, add_special_tokens=False, return_tensors="pt"
    ).to(device)
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs,
            max_new_tokens=250,
            do_sample=False,
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    response = response[len(formatted_prompt) :]  # remove input prompt from reponse
    response = response.replace("<eos>", "")  # remove eos token
    return response


question = "What is a transformer?"
print(inference(question=question, context=""))



from langchain.document_loaders import PyPDFLoader

loaders = [
    PyPDFLoader(r'/content/SRB-PART-I&PART-II-2024-25.pdf'),
    #PyPDFLoader(r'/_IVP_DCT_Prac.docx.pdf'),
]
pages = []
for loader in loaders:
    pages.extend(loader.load())



from langchain.text_splitter import TokenTextSplitter

text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=12)
docs = text_splitter.split_documents(pages)
print(docs[0].page_content)
import numpy as np
from langchain_community.embeddings import (
    HuggingFaceEmbeddings
)
encoder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': "cpu"})
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L12-v2", clean_up_tokenization_spaces=True)
embeddings1 = encoder.embed_query("RAG")
embeddings2 = encoder.embed_query(docs[0].page_content)
print(np.dot(embeddings1, embeddings2))



from langchain.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
faiss_db = FAISS.from_documents(docs, encoder, distance_strategy=DistanceStrategy.DOT_PRODUCT)
question = "Tell me about student dresscode"
retrieved_docs = faiss_db.similarity_search(question, k=5)
context = "".join(doc.page_content + "\n" for doc in retrieved_docs)
print(context)
